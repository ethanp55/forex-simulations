{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from pyts.image import RecurrencePlot, GramianAngularField\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "import json\n",
    "import time\n",
    "from collections import deque\n",
    "import warnings\n",
    "from pickle import dump, load\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "file_path = '/Users/mymac/Google Drive/My Drive/Forex_Robot/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currencies = ['Usd_Chf', 'Gbp_Usd', 'Usd_Cad', 'Aud_Usd', 'Eur_Usd', 'Usd_Jpy', 'Nzd_Usd']\n",
    "# currencies = ['Usd_Cad', 'Aud_Usd', 'Eur_Usd', 'Usd_Jpy', 'Gbp_Usd']\n",
    "currencies = ['Eur_Usd']\n",
    "years = '2012-2022'\n",
    "\n",
    "dfs, df_longs = [], []\n",
    "\n",
    "# news = pd.read_csv(file_path + 'events_2016-2022.csv')\n",
    "# news = news.rename(columns={'Start': 'Date'})\n",
    "# news.Date = pd.to_datetime(news.Date)\n",
    "\n",
    "for currency_pair in currencies:\n",
    "    # currency1, currency2 = currency_pair.split('_')\n",
    "    # currency1, currency2 = currency1.upper(), currency2.upper()\n",
    "\n",
    "    # curr_news = news.loc[(news['Currency'] == currency1) | (news['Currency'] == currency2)]\n",
    "    # curr_news.drop(['Id', 'Name', 'Currency'], axis=1, inplace=True)\n",
    "    # curr_news.drop(curr_news[(curr_news['Impact'] != 'LOW') & (curr_news['Impact'] != 'MEDIUM') & (curr_news['Impact'] != 'HIGH')].index, inplace=True)\n",
    "    # curr_news.loc[curr_news['Impact'] == 'LOW', 'Impact'] = 1\n",
    "    # curr_news.loc[curr_news['Impact'] == 'MEDIUM', 'Impact'] = 2\n",
    "    # curr_news.loc[curr_news['Impact'] == 'HIGH', 'Impact'] = 3\n",
    "    # curr_news = curr_news.groupby('Date')['Impact'].mean().reset_index()\n",
    "\n",
    "    df = pd.read_csv(file_path + f'Oanda_{currency_pair}_M5_{years}.csv')\n",
    "    df.Date = pd.to_datetime(df.Date)\n",
    "    # df.reset_index(drop=True, inplace=True)\n",
    "    # df = pd.merge(df, curr_news, how='left', on='Date')\n",
    "    # df.reset_index(drop=True, inplace=True)\n",
    "    # df = df.fillna(method='ffill')\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "    df_long = pd.read_csv(file_path + f'Oanda_{currency_pair}_M30_{years}.csv')\n",
    "    df_long.Date = pd.to_datetime(df_long.Date)\n",
    "    df_long.dropna(inplace=True)\n",
    "    df_long.reset_index(drop=True, inplace=True)\n",
    "    df_longs.append(df_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fractal(df, i, look_back=3):\n",
    "    if i >= look_back and i < df.shape[0] - look_back:\n",
    "        lows = []\n",
    "        highs = []\n",
    "\n",
    "        for j in range(1, look_back + 1):\n",
    "            prev_mid_low, prev_mid_high = df.loc[df.index[i - j], ['Mid_Low', 'Mid_High']]\n",
    "            future_mid_low, future_mid_high = df.loc[df.index[i + j], ['Mid_Low', 'Mid_High']]\n",
    "\n",
    "            lows.append(float(prev_mid_low))\n",
    "            lows.append(float(future_mid_low))\n",
    "            highs.append(float(prev_mid_high))\n",
    "            highs.append(float(future_mid_high))\n",
    "\n",
    "        mid_low, mid_high = df.loc[df.index[i], ['Mid_Low', 'Mid_High']]\n",
    "\n",
    "        if float(mid_low) < min(lows):\n",
    "            return float(mid_low), 1.0\n",
    "\n",
    "        elif float(mid_high) > max(highs):\n",
    "            return float(mid_high), 0.0\n",
    "\n",
    "        else:\n",
    "            return np.nan, np.nan\n",
    "\n",
    "    else:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def psar(barsdata, iaf=0.02, maxaf=0.2):\n",
    "    length = len(barsdata)\n",
    "    high = list(barsdata['Mid_High'])\n",
    "    low = list(barsdata['Mid_Low'])\n",
    "    close = list(barsdata['Mid_Close'])\n",
    "    psar = close[0:len(close)]\n",
    "    bull = True\n",
    "    af = iaf\n",
    "    hp = high[0]\n",
    "    lp = low[0]\n",
    "    for i in range(2, length):\n",
    "        if bull:\n",
    "            psar[i] = psar[i - 1] + af * (hp - psar[i - 1])\n",
    "        else:\n",
    "            psar[i] = psar[i - 1] + af * (lp - psar[i - 1])\n",
    "        reverse = False\n",
    "        if bull:\n",
    "            if low[i] < psar[i]:\n",
    "                bull = False\n",
    "                reverse = True\n",
    "                psar[i] = hp\n",
    "                lp = low[i]\n",
    "                af = iaf\n",
    "        else:\n",
    "            if high[i] > psar[i]:\n",
    "                bull = True\n",
    "                reverse = True\n",
    "                psar[i] = lp\n",
    "                hp = high[i]\n",
    "                af = iaf\n",
    "        if not reverse:\n",
    "            if bull:\n",
    "                if high[i] > hp:\n",
    "                    hp = high[i]\n",
    "                    af = min(af + iaf, maxaf)\n",
    "                if low[i - 1] < psar[i]:\n",
    "                    psar[i] = low[i - 1]\n",
    "                if low[i - 2] < psar[i]:\n",
    "                    psar[i] = low[i - 2]\n",
    "            else:\n",
    "                if low[i] < lp:\n",
    "                    lp = low[i]\n",
    "                    af = min(af + iaf, maxaf)\n",
    "                if high[i - 1] > psar[i]:\n",
    "                    psar[i] = high[i - 1]\n",
    "                if high[i - 2] > psar[i]:\n",
    "                    psar[i] = high[i - 2]\n",
    "    return psar\n",
    "\n",
    "\n",
    "def atr(high, low, close, lookback=14):\n",
    "    high_low = high - low\n",
    "    high_close = np.abs(high - close.shift())\n",
    "    low_close = np.abs(low - close.shift())\n",
    "    ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "    true_range = np.max(ranges, axis=1)\n",
    "\n",
    "    return true_range.rolling(lookback).sum() / lookback\n",
    "\n",
    "\n",
    "def rsi(closes, periods=14):\n",
    "    close_delta = closes.diff()\n",
    "\n",
    "    up = close_delta.clip(lower=0)\n",
    "    down = -1 * close_delta.clip(upper=0)\n",
    "    ma_up = up.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "    ma_down = down.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "        \n",
    "    rsi = ma_up / ma_down\n",
    "    rsi = 100 - (100/(1 + rsi))\n",
    "\n",
    "    return rsi\n",
    "\n",
    "  \n",
    "def adx(high, low, close, lookback=14):\n",
    "    plus_dm = high.diff()\n",
    "    minus_dm = low.diff()\n",
    "    plus_dm[plus_dm < 0] = 0\n",
    "    minus_dm[minus_dm > 0] = 0\n",
    "    \n",
    "    tr1 = pd.DataFrame(high - low)\n",
    "    tr2 = pd.DataFrame(abs(high - close.shift(1)))\n",
    "    tr3 = pd.DataFrame(abs(low - close.shift(1)))\n",
    "    frames = [tr1, tr2, tr3]\n",
    "    tr = pd.concat(frames, axis = 1, join = 'inner').max(axis = 1)\n",
    "    atr = tr.rolling(lookback).mean()\n",
    "    \n",
    "    plus_di = 100 * (plus_dm.ewm(alpha = 1/lookback).mean() / atr)\n",
    "    minus_di = abs(100 * (minus_dm.ewm(alpha = 1/lookback).mean() / atr))\n",
    "    dx = (abs(plus_di - minus_di) / abs(plus_di + minus_di)) * 100\n",
    "    adx = ((dx.shift(1) * (lookback - 1)) + dx) / lookback\n",
    "    adx_smooth = adx.ewm(alpha = 1/lookback).mean()\n",
    "\n",
    "    return adx_smooth\n",
    "\n",
    "\n",
    "def stoch(high, low, close, lookback=14):\n",
    "    high_lookback = high.rolling(lookback).max()\n",
    "    low_lookback = low.rolling(lookback).min()\n",
    "    slow_k = (close - low_lookback) * 100 / (high_lookback - low_lookback)\n",
    "    slow_d = slow_k.rolling(3).mean()\n",
    "\n",
    "    return slow_k, slow_d\n",
    "\n",
    "def stoch_rsi(data, k_window=3, d_window=3, window=14):\n",
    "    min_val = data.rolling(window=window, center=False).min()\n",
    "    max_val = data.rolling(window=window, center=False).max()\n",
    "\n",
    "    stoch = ((data - min_val) / (max_val - min_val)) * 100\n",
    "\n",
    "    slow_k = stoch.rolling(window=k_window, center=False).mean()\n",
    "\n",
    "    slow_d = slow_k.rolling(window=d_window, center=False).mean()\n",
    "\n",
    "    return slow_k, slow_d\n",
    "\n",
    "def n_macd(macd, macdsignal, lookback=50):\n",
    "    n_macd = 2 * (((macd - macd.rolling(lookback).min()) / (macd.rolling(lookback).max() - macd.rolling(lookback).min()))) - 1\n",
    "    n_macdsignal = 2 * (((macdsignal - macdsignal.rolling(lookback).min()) / (macdsignal.rolling(lookback).max() - macdsignal.rolling(lookback).min()))) - 1\n",
    "\n",
    "    return n_macd, n_macdsignal\n",
    "\n",
    "def chop(df, lookback=14):\n",
    "    atr1 = atr(df, lookback=1)\n",
    "    high, low = df['Mid_High'], df['Mid_Low']\n",
    "\n",
    "    chop = np.log10(atr1.rolling(lookback).sum() / (high.rolling(lookback).max() - low.rolling(lookback).min())) / np.log10(lookback)\n",
    "\n",
    "    return chop\n",
    "\n",
    "def vo(volume, short_lookback=5, long_lookback=10):\n",
    "    short_ema =  pd.Series.ewm(volume, span=short_lookback).mean()\n",
    "    long_ema = pd.Series.ewm(volume, span=long_lookback).mean()\n",
    "\n",
    "    volume_oscillator = (short_ema - long_ema) / long_ema\n",
    "\n",
    "    return volume_oscillator\n",
    "\n",
    "def bar_lengths(bar_lens, window=36):\n",
    "    return bar_lens.rolling(window=window).mean(), bar_lens.rolling(window=window).std()\n",
    "\n",
    "def sar_lengths(opens, sars, window=36):\n",
    "    diffs = abs(opens - sars.shift(1))\n",
    "\n",
    "    return diffs.rolling(window=window).mean(), diffs.rolling(window=window).std()\n",
    "\n",
    "def supertrend(barsdata, atr_len=3, mult=3):\n",
    "    curr_atr = atr(barsdata['Mid_High'], barsdata['Mid_Low'], barsdata['Mid_Close'], lookback=atr_len)\n",
    "    highs, lows = barsdata['Mid_High'], barsdata['Mid_Low']\n",
    "    hl2 = (highs + lows) / 2\n",
    "    final_upperband = upper_band = hl2 + mult * curr_atr\n",
    "    final_lowerband = lower_band = hl2 - mult * curr_atr\n",
    "\n",
    "    # initialize Supertrend column to True\n",
    "    supertrend = [True] * len(df)\n",
    "\n",
    "    close = barsdata['Mid_Close']\n",
    "    \n",
    "    for i in range(1, len(df.index)):\n",
    "        curr, prev = i, i - 1\n",
    "        \n",
    "        # if current close price crosses above upperband\n",
    "        if close[curr] > final_upperband[prev]:\n",
    "            supertrend[curr] = True\n",
    "\n",
    "        # if current close price crosses below lowerband\n",
    "        elif close[curr] < final_lowerband[prev]:\n",
    "            supertrend[curr] = False\n",
    "\n",
    "        # else, the trend continues\n",
    "        else:\n",
    "            supertrend[curr] = supertrend[prev]\n",
    "            \n",
    "            # adjustment to the final bands\n",
    "            if supertrend[curr] == True and final_lowerband[curr] < final_lowerband[prev]:\n",
    "                final_lowerband[curr] = final_lowerband[prev]\n",
    "\n",
    "            if supertrend[curr] == False and final_upperband[curr] > final_upperband[prev]:\n",
    "                final_upperband[curr] = final_upperband[prev]\n",
    "\n",
    "    return supertrend, final_upperband, final_lowerband\n",
    "\n",
    "# def supertrend(barsdata, atr_len=3, mult=3):\n",
    "#     curr_atr = atr(barsdata['ha_high'], barsdata['ha_low'], barsdata['ha_close'], lookback=atr_len)\n",
    "#     highs, lows = barsdata['ha_high'], barsdata['ha_low']\n",
    "#     hl2 = (highs + lows) / 2\n",
    "#     final_upperband = upper_band = hl2 + mult * curr_atr\n",
    "#     final_lowerband = lower_band = hl2 - mult * curr_atr\n",
    "\n",
    "#     # initialize Supertrend column to True\n",
    "#     supertrend = [True] * len(df)\n",
    "\n",
    "#     close = barsdata['ha_close']\n",
    "    \n",
    "#     for i in range(1, len(df.index)):\n",
    "#         curr, prev = i, i - 1\n",
    "        \n",
    "#         # if current close price crosses above upperband\n",
    "#         if close[curr] > final_upperband[prev]:\n",
    "#             supertrend[curr] = True\n",
    "\n",
    "#         # if current close price crosses below lowerband\n",
    "#         elif close[curr] < final_lowerband[prev]:\n",
    "#             supertrend[curr] = False\n",
    "\n",
    "#         # else, the trend continues\n",
    "#         else:\n",
    "#             supertrend[curr] = supertrend[prev]\n",
    "            \n",
    "#             # adjustment to the final bands\n",
    "#             if supertrend[curr] == True and final_lowerband[curr] < final_lowerband[prev]:\n",
    "#                 final_lowerband[curr] = final_lowerband[prev]\n",
    "\n",
    "#             if supertrend[curr] == False and final_upperband[curr] > final_upperband[prev]:\n",
    "#                 final_upperband[curr] = final_upperband[prev]\n",
    "\n",
    "#     return supertrend, final_upperband, final_lowerband\n",
    "\n",
    "def heikin_ashi(opens, highs, lows, closes):\n",
    "    ha_close = list((opens + highs + lows + closes) / 4)\n",
    "    ha_opens = []\n",
    "\n",
    "    opens_list, closes_list = list(opens), list(closes)\n",
    "\n",
    "    for i in range(len(ha_close)):\n",
    "        if i == 0:\n",
    "            ha_opens.append((opens_list[i] + closes_list[i]) / 2)\n",
    "\n",
    "        else:\n",
    "            ha_opens.append((ha_opens[i - 1] + ha_close[i - 1]) / 2)\n",
    "\n",
    "    ha_highs = list(pd.DataFrame({'ha_open': ha_opens, 'ha_close': ha_close, 'high': list(highs)}).max(axis=1))\n",
    "    ha_lows = list(pd.DataFrame({'ha_open': ha_opens, 'ha_close': ha_close, 'low': list(lows)}).min(axis=1))\n",
    "\n",
    "    return ha_opens, ha_highs, ha_lows, ha_close\n",
    "\n",
    "def trend_indicator(opens, highs, lows, closes, ema_period=50, smoothing_period=10):\n",
    "    ha_open, ha_high, ha_low, ha_close = heikin_ashi(opens, highs, lows, closes)\n",
    "\n",
    "    ha_o_ema = pd.Series.ewm(pd.DataFrame({'ha_open': ha_open}), span=ema_period).mean()\n",
    "    ha_h_ema = pd.Series.ewm(pd.DataFrame({'ha_high': ha_high}), span=ema_period).mean()\n",
    "    ha_l_ema = pd.Series.ewm(pd.DataFrame({'ha_low': ha_low}), span=ema_period).mean()\n",
    "    ha_c_ema = pd.Series.ewm(pd.DataFrame({'ha_close': ha_close}), span=ema_period).mean()\n",
    "\n",
    "    return pd.Series.ewm(ha_o_ema, span=smoothing_period).mean(), pd.Series.ewm(ha_h_ema, span=smoothing_period).mean(), pd.Series.ewm(ha_l_ema, span=smoothing_period).mean(), pd.Series.ewm(ha_c_ema, span=smoothing_period).mean()\n",
    "\n",
    "def qqe_mod(barsdata, rsi_period=6, smoothing=5, qqe_factor=5, qqe2_factor=1.61, threshold=3, mult=0.35, sma_length=50):\n",
    "    wilders_period = rsi_period * 2 - 1\n",
    "\n",
    "    curr_rsi = rsi(barsdata, periods=rsi_period)\n",
    "    rsi_ema = pd.Series.ewm(curr_rsi, span=smoothing).mean()\n",
    "    atr_rsi = abs(rsi_ema.shift(1) - rsi_ema)\n",
    "    atr_rsi_ema = pd.Series.ewm(atr_rsi, span=wilders_period).mean()\n",
    "    dar = pd.Series.ewm(atr_rsi_ema, span=wilders_period).mean() * qqe_factor\n",
    "\n",
    "    newshortband = rsi_ema + dar\n",
    "    newlongband = rsi_ema - dar\n",
    "\n",
    "    rsi_ema_list = list(rsi_ema)\n",
    "\n",
    "    longband = [0]\n",
    "    for i in range(1, len(rsi_ema_list)):\n",
    "        if rsi_ema_list[i - 1] > longband[i - 1] and rsi_ema_list[i] > longband[i - 1]:\n",
    "            longband.append(max(longband[i - 1],newlongband[i]))\n",
    "\n",
    "        else:\n",
    "            longband.append(newlongband[i])\n",
    "\n",
    "    shortband = [0]\n",
    "    for i in range(1,len(rsi_ema_list)):\n",
    "        if rsi_ema_list[i - 1] < shortband[i - 1] and rsi_ema_list[i] < shortband[i - 1]:\n",
    "            shortband.append(min(shortband[i - 1],newshortband[i]))\n",
    "            \n",
    "        else:\n",
    "            shortband.append(newshortband[i])\n",
    "\n",
    "    longband = pd.Series(longband)\n",
    "    shortband = pd.Series(shortband)\n",
    "\n",
    "    trend = np.where(rsi_ema > longband.shift(1), 1, -1)    \n",
    "    fastatrrsitl = pd.Series(np.where(trend == 1, longband, shortband))\n",
    "\n",
    "    basis = (fastatrrsitl - 50).rolling(window=sma_length).mean()\n",
    "    dev = (fastatrrsitl - 50).rolling(window=sma_length).std() * mult\n",
    "    upper = basis + dev\n",
    "    lower = basis - dev\n",
    "\n",
    "    greenbar1 = rsi_ema - 50 > threshold\n",
    "    greenbar2 = rsi_ema - 50 > upper\n",
    "    redbar1 = rsi_ema - 50 < threshold\n",
    "    redbar2 = rsi_ema - 50 < lower\n",
    "\n",
    "    # uptrend = np.where((greenbar1 & greenbar2), True, False)\n",
    "    # downtrend = np.where((redbar1 & redbar2), True, False)\n",
    "\n",
    "    uptrend = np.where((greenbar2), True, False)\n",
    "    downtrend = np.where((redbar2), True, False)\n",
    "\n",
    "    return uptrend, downtrend\n",
    "\n",
    "def williams_r(highs, lows, closes, length=21, ema_length=15):\n",
    "    highest_highs = highs.rolling(window=length).max()\n",
    "    lowest_lows = lows.rolling(window=length).min()\n",
    "\n",
    "    willy = 100 * (closes - highest_highs) / (highest_highs - lowest_lows)\n",
    "    willy_ema = pd.Series.ewm(willy, span=ema_length).mean()\n",
    "\n",
    "    return willy, willy_ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bid_ask_mid_prices_list = []\n",
    "\n",
    "for df in dfs:\n",
    "    # Add technical indicators (for additional features)\n",
    "    df['ema200'] = pd.Series.ewm(df['Mid_Close'], span=200).mean()\n",
    "    df['ema100'] = pd.Series.ewm(df['Mid_Close'], span=100).mean()\n",
    "\n",
    "    df['atr'] = atr(df['Mid_High'], df['Mid_Low'], df['Mid_Close'])\n",
    "    df['rsi'] = rsi(df['Mid_Close'])\n",
    "    df['adx'] = adx(df['Mid_High'], df['Mid_Low'], df['Mid_Close'])\n",
    "    df['macd'] = pd.Series.ewm(df['Mid_Close'], span=12).mean() - pd.Series.ewm(df['Mid_Close'], span=26).mean()\n",
    "    df['macdsignal'] = pd.Series.ewm(df['macd'], span=9).mean()\n",
    "    df['slowk_rsi'], df['slowd_rsi'] = stoch_rsi(df['rsi'])\n",
    "\n",
    "    tups = [add_fractal(df, i) for i in range(df.shape[0])]\n",
    "    key_levels, is_supports = [tup[0] for tup in tups], [tup[1] for tup in tups]\n",
    "    df['key_level'], df['is_support'] = key_levels, is_supports\n",
    "    df = df.fillna(method='ffill')\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Extract the bid and ask prices and fractals and remove them from the df\n",
    "    bid_ask_mid_prices = df[['Bid_Open', 'Bid_High', 'Bid_Low', 'Bid_Close', 'Ask_Open', 'Ask_High', 'Ask_Low', 'Ask_Close', 'Mid_Open', 'Mid_High', 'Mid_Low', 'Mid_Close']]\n",
    "    df.drop(['Bid_Open', 'Bid_High', 'Bid_Low', 'Bid_Close', 'Ask_Open', 'Ask_High', 'Ask_Low', 'Ask_Close', 'Mid_Open', 'Mid_High', 'Mid_Low', 'Mid_Close', 'Volume'], axis=1, inplace=True)\n",
    "    bid_ask_mid_prices_list.append(bid_ask_mid_prices)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for df_long in df_longs:\n",
    "    df_long['ema200'] = pd.Series.ewm(df_long['Mid_Close'], span=200).mean()\n",
    "    df_long['ema100'] = pd.Series.ewm(df_long['Mid_Close'], span=100).mean()\n",
    "\n",
    "    df_long['atr'] = atr(df_long['Mid_High'], df_long['Mid_Low'], df_long['Mid_Close'])\n",
    "    df_long['rsi'] = rsi(df_long['Mid_Close'])\n",
    "    df_long['adx'] = adx(df_long['Mid_High'], df_long['Mid_Low'], df_long['Mid_Close'])\n",
    "    df_long['macd'] = pd.Series.ewm(df_long['Mid_Close'], span=12).mean() - pd.Series.ewm(df_long['Mid_Close'], span=26).mean()\n",
    "    df_long['macdsignal'] = pd.Series.ewm(df_long['macd'], span=9).mean()\n",
    "    df_long['slowk_rsi'], df_long['slowd_rsi'] = stoch_rsi(df_long['rsi'])\n",
    "\n",
    "    tups = [add_fractal(df_long, i) for i in range(df_long.shape[0])]\n",
    "    key_levels, is_supports = [tup[0] for tup in tups], [tup[1] for tup in tups]\n",
    "    df_long['key_level'], df_long['is_support'] = key_levels, is_supports\n",
    "    df_long = df_long.fillna(method='ffill')\n",
    "\n",
    "    df_long.dropna(inplace=True)\n",
    "    df_long.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_long.drop(['Bid_Open', 'Bid_High', 'Bid_Low', 'Bid_Close', 'Ask_Open', 'Ask_High', 'Ask_Low', 'Ask_Close', 'Mid_Open', 'Mid_High', 'Mid_Low', 'Mid_Close', 'Volume'], axis=1, inplace=True)\n",
    "\n",
    "    df_long.dropna(inplace=True)\n",
    "    df_long.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    df, df_long = dfs[i], df_longs[i]\n",
    "\n",
    "    df = pd.merge(df, df_long, how='left', on='Date')\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = df.fillna(method='ffill')\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    dfs[i] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Bid_Open_x</th>\n",
       "      <th>Bid_High_x</th>\n",
       "      <th>Bid_Low_x</th>\n",
       "      <th>Bid_Close_x</th>\n",
       "      <th>Ask_Open_x</th>\n",
       "      <th>Ask_High_x</th>\n",
       "      <th>Ask_Low_x</th>\n",
       "      <th>Ask_Close_x</th>\n",
       "      <th>Mid_Open_x</th>\n",
       "      <th>...</th>\n",
       "      <th>ema100_y</th>\n",
       "      <th>atr_y</th>\n",
       "      <th>rsi_y</th>\n",
       "      <th>adx_y</th>\n",
       "      <th>macd_y</th>\n",
       "      <th>macdsignal_y</th>\n",
       "      <th>slowk_rsi_y</th>\n",
       "      <th>slowd_rsi_y</th>\n",
       "      <th>key_level_y</th>\n",
       "      <th>is_support_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-01-02 19:00:00</td>\n",
       "      <td>1.3564</td>\n",
       "      <td>1.3564</td>\n",
       "      <td>1.3564</td>\n",
       "      <td>1.3564</td>\n",
       "      <td>1.3574</td>\n",
       "      <td>1.3574</td>\n",
       "      <td>1.3574</td>\n",
       "      <td>1.3574</td>\n",
       "      <td>1.3569</td>\n",
       "      <td>...</td>\n",
       "      <td>1.298901</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>61.639766</td>\n",
       "      <td>30.350035</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.010028</td>\n",
       "      <td>37.370852</td>\n",
       "      <td>51.886872</td>\n",
       "      <td>1.357</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-01-02 19:05:00</td>\n",
       "      <td>1.3565</td>\n",
       "      <td>1.3565</td>\n",
       "      <td>1.3564</td>\n",
       "      <td>1.3564</td>\n",
       "      <td>1.3575</td>\n",
       "      <td>1.3575</td>\n",
       "      <td>1.3574</td>\n",
       "      <td>1.3574</td>\n",
       "      <td>1.3570</td>\n",
       "      <td>...</td>\n",
       "      <td>1.298901</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>61.639766</td>\n",
       "      <td>30.350035</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.010028</td>\n",
       "      <td>37.370852</td>\n",
       "      <td>51.886872</td>\n",
       "      <td>1.357</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-01-02 19:10:00</td>\n",
       "      <td>1.3564</td>\n",
       "      <td>1.3564</td>\n",
       "      <td>1.3564</td>\n",
       "      <td>1.3564</td>\n",
       "      <td>1.3574</td>\n",
       "      <td>1.3574</td>\n",
       "      <td>1.3574</td>\n",
       "      <td>1.3574</td>\n",
       "      <td>1.3569</td>\n",
       "      <td>...</td>\n",
       "      <td>1.298901</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>61.639766</td>\n",
       "      <td>30.350035</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.010028</td>\n",
       "      <td>37.370852</td>\n",
       "      <td>51.886872</td>\n",
       "      <td>1.357</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-01-02 19:15:00</td>\n",
       "      <td>1.3560</td>\n",
       "      <td>1.3564</td>\n",
       "      <td>1.3559</td>\n",
       "      <td>1.3561</td>\n",
       "      <td>1.3570</td>\n",
       "      <td>1.3574</td>\n",
       "      <td>1.3569</td>\n",
       "      <td>1.3571</td>\n",
       "      <td>1.3565</td>\n",
       "      <td>...</td>\n",
       "      <td>1.298901</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>61.639766</td>\n",
       "      <td>30.350035</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.010028</td>\n",
       "      <td>37.370852</td>\n",
       "      <td>51.886872</td>\n",
       "      <td>1.357</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-01-02 19:20:00</td>\n",
       "      <td>1.3559</td>\n",
       "      <td>1.3563</td>\n",
       "      <td>1.3553</td>\n",
       "      <td>1.3554</td>\n",
       "      <td>1.3569</td>\n",
       "      <td>1.3573</td>\n",
       "      <td>1.3563</td>\n",
       "      <td>1.3564</td>\n",
       "      <td>1.3564</td>\n",
       "      <td>...</td>\n",
       "      <td>1.298901</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>61.639766</td>\n",
       "      <td>30.350035</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.010028</td>\n",
       "      <td>37.370852</td>\n",
       "      <td>51.886872</td>\n",
       "      <td>1.357</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  Bid_Open_x  Bid_High_x  Bid_Low_x  Bid_Close_x  \\\n",
       "0 2005-01-02 19:00:00      1.3564      1.3564     1.3564       1.3564   \n",
       "1 2005-01-02 19:05:00      1.3565      1.3565     1.3564       1.3564   \n",
       "2 2005-01-02 19:10:00      1.3564      1.3564     1.3564       1.3564   \n",
       "3 2005-01-02 19:15:00      1.3560      1.3564     1.3559       1.3561   \n",
       "4 2005-01-02 19:20:00      1.3559      1.3563     1.3553       1.3554   \n",
       "\n",
       "   Ask_Open_x  Ask_High_x  Ask_Low_x  Ask_Close_x  Mid_Open_x  ...  ema100_y  \\\n",
       "0      1.3574      1.3574     1.3574       1.3574      1.3569  ...  1.298901   \n",
       "1      1.3575      1.3575     1.3574       1.3574      1.3570  ...  1.298901   \n",
       "2      1.3574      1.3574     1.3574       1.3574      1.3569  ...  1.298901   \n",
       "3      1.3570      1.3574     1.3569       1.3571      1.3565  ...  1.298901   \n",
       "4      1.3569      1.3573     1.3563       1.3564      1.3564  ...  1.298901   \n",
       "\n",
       "      atr_y      rsi_y      adx_y    macd_y  macdsignal_y  slowk_rsi_y  \\\n",
       "0  0.003416  61.639766  30.350035  0.009487      0.010028    37.370852   \n",
       "1  0.003416  61.639766  30.350035  0.009487      0.010028    37.370852   \n",
       "2  0.003416  61.639766  30.350035  0.009487      0.010028    37.370852   \n",
       "3  0.003416  61.639766  30.350035  0.009487      0.010028    37.370852   \n",
       "4  0.003416  61.639766  30.350035  0.009487      0.010028    37.370852   \n",
       "\n",
       "   slowd_rsi_y  key_level_y  is_support_y  \n",
       "0    51.886872        1.357           0.0  \n",
       "1    51.886872        1.357           0.0  \n",
       "2    51.886872        1.357           0.0  \n",
       "3    51.886872        1.357           0.0  \n",
       "4    51.886872        1.357           0.0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back_size = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_image_data(subset):\n",
    "  # rp_transformer = RecurrencePlot()\n",
    "  # rp_subset = rp_transformer.transform(subset)\n",
    "\n",
    "  # return rp_subset\n",
    "\n",
    "  gasf_transformer = GramianAngularField(method='summation')\n",
    "  gasf_subset = gasf_transformer.transform(subset)\n",
    "\n",
    "  return gasf_subset\n",
    "\n",
    "  # gadf_transformer = GramianAngularField(method='difference')\n",
    "  # gadf_subset = gadf_transformer.transform(subset)\n",
    "\n",
    "  # return gadf_subset\n",
    "\n",
    "  # image_data = np.append(rp_subset, gasf_subset, axis=-1)\n",
    "  # image_data = np.append(image_data, gadf_subset ,axis=-1)\n",
    "  \n",
    "  # return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING SIMULATION FOR Eur_Usd...\n",
      "2005\n",
      "Buys: 0\n",
      "Sells: 0\n",
      "Nones: 0\n",
      "\n",
      "2006\n",
      "Buys: 20\n",
      "Sells: 12\n",
      "Nones: 53\n",
      "\n",
      "2007\n",
      "Buys: 33\n",
      "Sells: 25\n",
      "Nones: 87\n",
      "\n",
      "2008\n",
      "Buys: 43\n",
      "Sells: 38\n",
      "Nones: 114\n",
      "\n",
      "2009\n",
      "Buys: 124\n",
      "Sells: 119\n",
      "Nones: 372\n",
      "\n",
      "2010\n",
      "Buys: 192\n",
      "Sells: 181\n",
      "Nones: 630\n",
      "\n",
      "2011\n",
      "Buys: 233\n",
      "Sells: 245\n",
      "Nones: 780\n",
      "\n",
      "2012\n",
      "Buys: 296\n",
      "Sells: 318\n",
      "Nones: 965\n",
      "\n",
      "2013\n",
      "Buys: 314\n",
      "Sells: 336\n",
      "Nones: 1036\n",
      "\n",
      "2014\n",
      "Buys: 327\n",
      "Sells: 359\n",
      "Nones: 1103\n",
      "\n",
      "2015\n",
      "Buys: 332\n",
      "Sells: 370\n",
      "Nones: 1132\n",
      "\n",
      "2016\n",
      "Buys: 356\n",
      "Sells: 405\n",
      "Nones: 1214\n",
      "\n",
      "2017\n",
      "Buys: 366\n",
      "Sells: 419\n",
      "Nones: 1249\n",
      "\n",
      "2018\n",
      "Buys: 374\n",
      "Sells: 422\n",
      "Nones: 1274\n",
      "\n",
      "2019\n",
      "Buys: 381\n",
      "Sells: 428\n",
      "Nones: 1305\n",
      "\n",
      "2020\n",
      "Buys: 383\n",
      "Sells: 433\n",
      "Nones: 1316\n",
      "\n",
      "2021\n",
      "Buys: 396\n",
      "Sells: 445\n",
      "Nones: 1358\n",
      "\n",
      "2022\n",
      "Buys: 403\n",
      "Sells: 449\n",
      "Nones: 1364\n",
      "\n",
      "Buys: 411\n",
      "Sells: 456\n",
      "Nones: 1386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buys_list = []\n",
    "sells_list = []\n",
    "nones_list = []\n",
    "\n",
    "value_per_pip = 1.0\n",
    "amounts_per_day = [-0.00008, -0.0001, -0.00012]\n",
    "spread_cutoff = 0.10\n",
    "risk_reward_ratio = 1.5\n",
    "each_bar = False\n",
    "n_bars = 3\n",
    "pip_movement = 20\n",
    "use_pullback = True\n",
    "lookback = n_bars + 1 if use_pullback else n_bars\n",
    "lookforward = -1 if use_pullback else 0\n",
    "\n",
    "def get_n_units(trade_type, stop_loss, ask_open, bid_open, mid_open, currency_pair):\n",
    "    _, second = currency_pair.split('_')\n",
    "  \n",
    "    pips_to_risk = ask_open - stop_loss if trade_type == 'buy' else stop_loss - bid_open\n",
    "    pips_to_risk_calc = pips_to_risk * 10000 if second != 'Jpy' else pips_to_risk * 100\n",
    "\n",
    "    if second == 'Usd':\n",
    "        per_pip = 0.0001\n",
    "\n",
    "    else:\n",
    "        per_pip = 0.0001 / mid_open if second != 'Jpy' else 0.01 / mid_open\n",
    "\n",
    "    n_units = int(50 / (pips_to_risk_calc * per_pip))\n",
    "\n",
    "    return n_units\n",
    "\n",
    "def calculate_day_fees(start_date, end_date, n_units):\n",
    "    curr_fee = np.random.choice(amounts_per_day, p=[0.25, 0.50, 0.25]) * n_units\n",
    "    num_days = np.busday_count(start_date.date(), end_date.date())\n",
    "\n",
    "    return num_days * curr_fee\n",
    "\n",
    "for idx in range(len(currencies)):\n",
    "    currency_pair, df, df_long, bid_ask_mid_prices = currencies[idx], dfs[idx], df_longs[idx], bid_ask_mid_prices_list[idx]\n",
    "    rounding = 3 if 'Jpy' in currency_pair else 5\n",
    "    pip_movement_to_use = pip_movement / 100 if 'Jpy' in currency_pair else pip_movement / 10000\n",
    "    buys, sells, nones = [], [], []\n",
    "    trade, prev_year = None, None\n",
    "\n",
    "    print(f'RUNNING SIMULATION FOR {currency_pair}...')\n",
    "\n",
    "    for i in range(look_back_size, len(df)):\n",
    "        curr_date = df.loc[df.index[i], 'Date']\n",
    "\n",
    "        if prev_year is None or curr_date.year > prev_year:\n",
    "            prev_year = curr_date.year\n",
    "            print(prev_year)\n",
    "            print(f'Buys: {len(buys)}')\n",
    "            print(f'Sells: {len(sells)}')\n",
    "            print(f'Nones: {len(nones)}\\n')\n",
    "\n",
    "        curr_bid_open, curr_bid_high, curr_bid_low, curr_ask_open, curr_ask_high, curr_ask_low, curr_mid_open = bid_ask_mid_prices.loc[bid_ask_mid_prices.index[i], ['Bid_Open', 'Bid_High', 'Bid_Low', 'Ask_Open', 'Ask_High', 'Ask_Low', 'Mid_Open']]\n",
    "        spread = abs(curr_ask_open - curr_bid_open)\n",
    "\n",
    "        mid_opens = list(bid_ask_mid_prices.loc[bid_ask_mid_prices.index[i - lookback:i + lookforward], 'Mid_Open'])\n",
    "        mid_highs = list(bid_ask_mid_prices.loc[bid_ask_mid_prices.index[i - lookback:i + lookforward], 'Mid_High'])\n",
    "        mid_lows = list(bid_ask_mid_prices.loc[bid_ask_mid_prices.index[i - lookback:i + lookforward], 'Mid_Low'])\n",
    "        mid_closes = list(bid_ask_mid_prices.loc[bid_ask_mid_prices.index[i - lookback:i + lookforward], 'Mid_Close'])\n",
    "\n",
    "        if each_bar:\n",
    "            buy_signal = all([mid_opens[j] < mid_closes[j] and abs(mid_opens[j] - mid_closes[j]) >= pip_movement_to_use for j in range(len(mid_opens))])\n",
    "            sell_signal = all([mid_opens[j] > mid_closes[j] and abs(mid_opens[j] - mid_closes[j]) >= pip_movement_to_use for j in range(len(mid_opens))])\n",
    "\n",
    "        else:\n",
    "            buy_signal = all([mid_opens[j] < mid_closes[j] for j in range(len(mid_opens))]) and abs(mid_opens[0] - mid_closes[-1]) >= pip_movement_to_use\n",
    "            sell_signal = all([mid_opens[j] > mid_closes[j] for j in range(len(mid_opens))]) and abs(mid_opens[0] - mid_closes[-1]) >= pip_movement_to_use\n",
    "\n",
    "        if use_pullback and buy_signal:\n",
    "            mid_open1, mid_high1, mid_low1, mid_close1 = bid_ask_mid_prices.loc[bid_ask_mid_prices.index[i - 1], ['Mid_Open', 'Mid_High', 'Mid_Low', 'Mid_Close']]\n",
    "            buy_signal = mid_open1 > mid_close1 and abs(mid_close1 - mid_open1) <= 0.25 * abs(mid_high1 - mid_low1)\n",
    "\n",
    "        if use_pullback and sell_signal:\n",
    "            mid_open1, mid_high1, mid_low1, mid_close1 = bid_ask_mid_prices.loc[bid_ask_mid_prices.index[i - 1], ['Mid_Open', 'Mid_High', 'Mid_Low', 'Mid_Close']]\n",
    "            sell_signal = mid_open1 < mid_close1 and abs(mid_close1 - mid_open1) <= 0.25 * abs(mid_high1 - mid_low1)\n",
    "\n",
    "        highest_high, lowest_low = max(mid_highs), min(mid_lows)\n",
    "\n",
    "        if trade is None:\n",
    "            if buy_signal:\n",
    "                open_price = float(curr_ask_open)\n",
    "                stop_loss = round(lowest_low, rounding)\n",
    "\n",
    "                if stop_loss < open_price:\n",
    "                    curr_pips_to_risk = open_price - stop_loss\n",
    "\n",
    "                    if spread <= curr_pips_to_risk * spread_cutoff:\n",
    "                        stop_gain = round(open_price + (curr_pips_to_risk * risk_reward_ratio), rounding)\n",
    "\n",
    "                        n_units = get_n_units('buy', stop_loss, curr_ask_open, curr_bid_open, curr_mid_open, currency_pair)\n",
    "\n",
    "                        trade = {'open_price': open_price, 'trade_type': 'buy', 'stop_loss': stop_loss,\n",
    "                                                        'stop_gain': stop_gain, 'pips_risked': round(curr_pips_to_risk, 5),\n",
    "                                                        'n_units': n_units, 'original_units': n_units, 'start_date': curr_date, 'end_date': None}\n",
    "\n",
    "            elif sell_signal:\n",
    "                open_price = float(curr_bid_open)\n",
    "                stop_loss = round(highest_high, rounding)\n",
    "\n",
    "                if stop_loss > open_price:\n",
    "                    curr_pips_to_risk = stop_loss - open_price\n",
    "\n",
    "                    if spread <= curr_pips_to_risk * spread_cutoff:\n",
    "                        stop_gain = round(open_price - (curr_pips_to_risk * risk_reward_ratio), rounding)\n",
    "\n",
    "                        n_units = get_n_units('sell', stop_loss, curr_ask_open, curr_bid_open, curr_mid_open, currency_pair)\n",
    "\n",
    "                        trade = {'open_price': open_price, 'trade_type': 'sell', 'stop_loss': stop_loss,\n",
    "                                'stop_gain': stop_gain, 'pips_risked': round(curr_pips_to_risk, 5),\n",
    "                                'n_units': n_units, 'original_units': n_units, 'start_date': curr_date, 'end_date': None}\n",
    "\n",
    "        if trade is not None:\n",
    "            for j in range(i, len(df)):\n",
    "                curr_date = df.loc[df.index[j], 'Date']\n",
    "                curr_bid_open, curr_bid_high, curr_bid_low, curr_bid_close, curr_ask_open, curr_ask_high, curr_ask_low, curr_ask_close = bid_ask_mid_prices.loc[bid_ask_mid_prices.index[j], ['Bid_Open', 'Bid_High', 'Bid_Low', 'Bid_Close', 'Ask_Open', 'Ask_High', 'Ask_Low', 'Ask_Close']]\n",
    "\n",
    "                if trade['trade_type'] == 'buy' and curr_bid_low <= trade['stop_loss']:\n",
    "                    nones.append(trade['start_date']) \n",
    "\n",
    "                    trade = None\n",
    "                    break\n",
    "\n",
    "\n",
    "                if trade['trade_type'] == 'buy' and curr_bid_high >= trade['stop_gain']:\n",
    "                    trade_amount = (trade['stop_gain'] - trade['open_price']) * trade['n_units'] * value_per_pip\n",
    "                    day_fees = calculate_day_fees(trade['start_date'], curr_date, trade['n_units'])\n",
    "\n",
    "                    if trade_amount + day_fees > 0:\n",
    "                        buys.append(trade['start_date'])\n",
    "\n",
    "                    trade = None\n",
    "                    break\n",
    "\n",
    "                if trade['trade_type'] == 'sell' and curr_ask_high >= trade['stop_loss']:\n",
    "                    nones.append(trade['start_date'])\n",
    "\n",
    "                    trade = None\n",
    "                    break\n",
    "\n",
    "                if trade['trade_type'] == 'sell' and curr_ask_low <= trade['stop_gain']:\n",
    "                    trade_amount = (trade['open_price'] - trade['stop_gain']) * trade['n_units'] * value_per_pip\n",
    "                    day_fees = calculate_day_fees(trade['start_date'], curr_date, trade['n_units'])\n",
    "\n",
    "                    if trade_amount + day_fees > 0:\n",
    "                        sells.append(trade['start_date'])\n",
    "\n",
    "                    trade = None\n",
    "                    break\n",
    "    \n",
    "    buys_list.append(buys)\n",
    "    sells_list.append(sells)\n",
    "    nones_list.append(nones)\n",
    "\n",
    "    print(f'Buys: {len(buys)}')\n",
    "    print(f'Sells: {len(sells)}')\n",
    "    print(f'Nones: {len(nones)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411\n",
      "456\n",
      "1386\n"
     ]
    }
   ],
   "source": [
    "buy_indices_list = []\n",
    "sell_indices_list = []\n",
    "nones_indices_list = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    buys, sells, nones, df = buys_list[i], sells_list[i], nones_list[i], dfs[i]\n",
    "    \n",
    "    buy_indices = [df.index[df['Date'] == curr_date] - 1 for curr_date in buys]\n",
    "    sell_indices = [df.index[df['Date'] == curr_date] - 1 for curr_date in sells]\n",
    "    nones_indices = [df.index[df['Date'] == curr_date] - 1 for curr_date in nones]\n",
    "\n",
    "    buy_indices_list.append(buy_indices)\n",
    "    sell_indices_list.append(sell_indices)\n",
    "    nones_indices_list.append(nones_indices)\n",
    "\n",
    "    print(len(buy_indices))\n",
    "    print(len(sell_indices))\n",
    "    print(len(nones_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    df.drop(['Bid_Open_x', 'Bid_High_x', 'Bid_Low_x', 'Bid_Close_x', 'Ask_Open_x', 'Ask_High_x', 'Ask_Low_x', 'Ask_Close_x', 'Mid_Open_x', 'Mid_High_x', 'Mid_Low_x', 'Mid_Close_x', 'Volume_x'], axis=1, inplace=True)\n",
    "    df.drop(['Bid_Open_y', 'Bid_High_y', 'Bid_Low_y', 'Bid_Close_y', 'Ask_Open_y', 'Ask_High_y', 'Ask_Low_y', 'Ask_Close_y', 'Mid_Open_y', 'Mid_High_y', 'Mid_Low_y', 'Mid_Close_y', 'Volume_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 22, 22)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i = 5000\n",
    "# foo = dfs[0].iloc[i - look_back_size + 1:i + 1, 1:]\n",
    "# curr_date = dfs[0].iloc[i, 0]\n",
    "# curr_long = df_longs[0].loc[df_longs[0]['Date'] < curr_date]\n",
    "# foo2 = curr_long.iloc[-look_back_size - 1:-1, 1:]\n",
    "# foo3 = pd.concat([foo.reset_index(drop=True), foo2.reset_index(drop=True)], axis=1, ignore_index=True)\n",
    "# correct_shape = grab_image_data(foo3).shape\n",
    "# correct_shape\n",
    "\n",
    "i = 5000\n",
    "foo = dfs[0].iloc[i - look_back_size + 1:i + 1, 1:]\n",
    "correct_shape = grab_image_data(foo).shape\n",
    "correct_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sequential_data():\n",
    "#     no_actions = []\n",
    "#     buys = []\n",
    "#     sells = []\n",
    "\n",
    "#     for z in range(len(dfs)):\n",
    "#         df, df_long = dfs[z], df_longs[z]\n",
    "\n",
    "#         buy_indices, sell_indices, nones_indices = buy_indices_list[z], sell_indices_list[z], nones_indices_list[z]\n",
    "\n",
    "#         for i in buy_indices:\n",
    "#             if len(i) == 1:\n",
    "#                 i = i[0]\n",
    "#                 seq1 = df.iloc[i - look_back_size + 1:i + 1, 1:]\n",
    "#                 curr_date = df.iloc[i, 0]\n",
    "#                 curr_long = df_long.loc[df_long['Date'] < curr_date]\n",
    "#                 seq2 = curr_long.iloc[-look_back_size - 1:-1, 1:]\n",
    "#                 seq = pd.concat([seq1.reset_index(drop=True), seq2.reset_index(drop=True)], axis=1, ignore_index=True)\n",
    "\n",
    "#                 if seq.shape == correct_shape[:-1] and not seq.isnull().values.any():\n",
    "#                     seq = grab_image_data(seq)\n",
    "#                     buys.append([seq, np.array([0, 1, 0])])\n",
    "\n",
    "#         for i in sell_indices:\n",
    "#             if len(i) == 1:\n",
    "#                 i = i[0]\n",
    "#                 seq1 = df.iloc[i - look_back_size + 1:i + 1, 1:]\n",
    "#                 curr_date = df.iloc[i, 0]\n",
    "#                 curr_long = df_long.loc[df_long['Date'] < curr_date]\n",
    "#                 seq2 = curr_long.iloc[-look_back_size - 1:-1, 1:]\n",
    "#                 seq = pd.concat([seq1.reset_index(drop=True), seq2.reset_index(drop=True)], axis=1, ignore_index=True)\n",
    "\n",
    "#                 if seq.shape == correct_shape[:-1] and not seq.isnull().values.any():\n",
    "#                     seq = grab_image_data(seq)\n",
    "#                     sells.append([seq, np.array([0, 0, 1])])\n",
    "\n",
    "#         for i in nones_indices:\n",
    "#             if len(i) == 1:\n",
    "#                 i = i[0]\n",
    "#                 seq1 = df.iloc[i - look_back_size + 1:i + 1, 1:]\n",
    "#                 curr_date = df.iloc[i, 0]\n",
    "#                 curr_long = df_long.loc[df_long['Date'] < curr_date]\n",
    "#                 seq2 = curr_long.iloc[-look_back_size - 1:-1, 1:]\n",
    "#                 seq = pd.concat([seq1.reset_index(drop=True), seq2.reset_index(drop=True)], axis=1, ignore_index=True)\n",
    "\n",
    "#                 if seq.shape == correct_shape[:-1] and not seq.isnull().values.any():\n",
    "#                     seq = grab_image_data(seq)\n",
    "#                     no_actions.append([seq, np.array([1, 0, 0])])\n",
    "\n",
    "#     np.random.shuffle(no_actions)\n",
    "#     np.random.shuffle(buys)\n",
    "#     np.random.shuffle(sells)\n",
    "\n",
    "#     lower = min(len(no_actions), len(buys), len(sells))\n",
    "\n",
    "#     no_actions = no_actions[:int(lower * 1.2)]\n",
    "\n",
    "#     sequential_data = no_actions + buys + sells\n",
    "#     np.random.shuffle(sequential_data)\n",
    "\n",
    "#     return sequential_data\n",
    "\n",
    "def get_sequential_data():\n",
    "    no_actions = []\n",
    "    buys = []\n",
    "    sells = []\n",
    "\n",
    "    for z in range(len(dfs)):\n",
    "        df, df_long = dfs[z], df_longs[z]\n",
    "\n",
    "        buy_indices, sell_indices, nones_indices = buy_indices_list[z], sell_indices_list[z], nones_indices_list[z]\n",
    "\n",
    "        for i in buy_indices:\n",
    "            if len(i) == 1:\n",
    "                i = i[0]\n",
    "                seq = df.iloc[i - look_back_size + 1:i + 1, 1:]\n",
    "\n",
    "                if seq.shape == correct_shape[:-1] and not seq.isnull().values.any():\n",
    "                    seq = grab_image_data(seq)\n",
    "                    buys.append([seq, np.array([0, 1, 0])])\n",
    "\n",
    "        for i in sell_indices:\n",
    "            if len(i) == 1:\n",
    "                i = i[0]\n",
    "                seq = df.iloc[i - look_back_size + 1:i + 1, 1:]\n",
    "\n",
    "                if seq.shape == correct_shape[:-1] and not seq.isnull().values.any():\n",
    "                    seq = grab_image_data(seq)\n",
    "                    sells.append([seq, np.array([0, 0, 1])])\n",
    "\n",
    "        for i in nones_indices:\n",
    "            if len(i) == 1:\n",
    "                i = i[0]\n",
    "                seq = df.iloc[i - look_back_size + 1:i + 1, 1:]\n",
    "\n",
    "                if seq.shape == correct_shape[:-1] and not seq.isnull().values.any():\n",
    "                    seq = grab_image_data(seq)\n",
    "                    no_actions.append([seq, np.array([1, 0, 0])])\n",
    "\n",
    "    np.random.shuffle(no_actions)\n",
    "    np.random.shuffle(buys)\n",
    "    np.random.shuffle(sells)\n",
    "\n",
    "    lower = min(len(no_actions), len(buys), len(sells))\n",
    "\n",
    "    no_actions = no_actions[:int(lower * 1.1)]\n",
    "\n",
    "    sequential_data = no_actions + buys + sells\n",
    "    np.random.shuffle(sequential_data)\n",
    "\n",
    "    return sequential_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_data = get_sequential_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1319"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequential_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "923\n",
      "396\n"
     ]
    }
   ],
   "source": [
    "training_proportion = 0.70\n",
    "train_test_cutoff_index = int(len(sequential_data) * training_proportion)\n",
    "\n",
    "train_set = sequential_data[0:train_test_cutoff_index]\n",
    "test_set = sequential_data[train_test_cutoff_index:]\n",
    "\n",
    "print('Dataset shapes:')\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for seq, target in train_set:\n",
    "  x_train.append(seq)\n",
    "  y_train.append(target)\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for seq, target in test_set:\n",
    "  x_test.append(seq)\n",
    "  y_test.append(target)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(923, 250, 22, 22)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(923, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of possible actions to take - determines the output dimension of the\n",
    "# #  neural network\n",
    "# n_actions = 3\n",
    "# input_data_shape = x_train.shape[1:]\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(filters = 32, kernel_size = (3,3), padding ='Same', activation ='relu', input_shape = input_data_shape))\n",
    "# model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu'))\n",
    "\n",
    "# model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation = \"relu\"))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(n_actions, activation = \"softmax\"))\n",
    "\n",
    "n_actions = 3\n",
    "input_data_shape = x_train.shape[1:]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3,3), padding ='Same', activation ='relu', input_shape = input_data_shape))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu'))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(128, activation = \"relu\"))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(128, activation = \"relu\"))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "# model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dense(n_actions, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 500\n",
    "batch_size = 32\n",
    "n_steps = len(x_train) // batch_size \n",
    "mean_loss = tf.keras.metrics.Mean() \n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.categorical_crossentropy\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "# loss_fn = tf.keras.losses.MeanAbsoluteError\n",
    "# metrics = [tf.keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_accuracy', verbose=1, patience=n_epochs)\n",
    "model_checkpoint = ModelCheckpoint(f'/Users/mymac/forex_bar_movement_cnn_{currencies[0]}', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "optimizer = Adam()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.2091 - accuracy: 0.3499\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.32071, saving model to /Users/mymac/forex_bar_movement_cnn_Eur_Usd\n",
      "INFO:tensorflow:Assets written to: /Users/mymac/forex_bar_movement_cnn_Eur_Usd/assets\n",
      "29/29 [==============================] - 3s 103ms/step - loss: 2.2091 - accuracy: 0.3499 - val_loss: 1.0988 - val_accuracy: 0.3207\n",
      "Epoch 2/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0984 - accuracy: 0.3521\n",
      "Epoch 00002: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0984 - accuracy: 0.3521 - val_loss: 1.0988 - val_accuracy: 0.3207\n",
      "Epoch 3/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0982 - accuracy: 0.3521\n",
      "Epoch 00003: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0982 - accuracy: 0.3521 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 4/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0981 - accuracy: 0.3521\n",
      "Epoch 00004: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0981 - accuracy: 0.3521 - val_loss: 1.0986 - val_accuracy: 0.3207\n",
      "Epoch 5/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0979 - accuracy: 0.3521\n",
      "Epoch 00005: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0979 - accuracy: 0.3521 - val_loss: 1.0986 - val_accuracy: 0.3207\n",
      "Epoch 6/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0978 - accuracy: 0.3521\n",
      "Epoch 00006: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 66ms/step - loss: 1.0978 - accuracy: 0.3521 - val_loss: 1.0988 - val_accuracy: 0.3207\n",
      "Epoch 7/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0977 - accuracy: 0.3521\n",
      "Epoch 00007: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0977 - accuracy: 0.3521 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 8/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0976 - accuracy: 0.3521\n",
      "Epoch 00008: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0976 - accuracy: 0.3521 - val_loss: 1.0986 - val_accuracy: 0.3207\n",
      "Epoch 9/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0976 - accuracy: 0.3521\n",
      "Epoch 00009: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0976 - accuracy: 0.3521 - val_loss: 1.0986 - val_accuracy: 0.3207\n",
      "Epoch 10/500\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 1.0975 - accuracy: 0.3504\n",
      "Epoch 00010: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0975 - accuracy: 0.3521 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 11/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0974 - accuracy: 0.3521\n",
      "Epoch 00011: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0974 - accuracy: 0.3521 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 12/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0974 - accuracy: 0.3521\n",
      "Epoch 00012: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0974 - accuracy: 0.3521 - val_loss: 1.0987 - val_accuracy: 0.3207\n",
      "Epoch 13/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0974 - accuracy: 0.3521\n",
      "Epoch 00013: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0974 - accuracy: 0.3521 - val_loss: 1.0988 - val_accuracy: 0.3207\n",
      "Epoch 14/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0974 - accuracy: 0.3521\n",
      "Epoch 00014: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0974 - accuracy: 0.3521 - val_loss: 1.0988 - val_accuracy: 0.3207\n",
      "Epoch 15/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0973 - accuracy: 0.3521\n",
      "Epoch 00015: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0973 - accuracy: 0.3521 - val_loss: 1.0988 - val_accuracy: 0.3207\n",
      "Epoch 16/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0974 - accuracy: 0.3521\n",
      "Epoch 00016: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0974 - accuracy: 0.3521 - val_loss: 1.0989 - val_accuracy: 0.3207\n",
      "Epoch 17/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0973 - accuracy: 0.3521\n",
      "Epoch 00017: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0973 - accuracy: 0.3521 - val_loss: 1.0988 - val_accuracy: 0.3207\n",
      "Epoch 18/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0973 - accuracy: 0.3521\n",
      "Epoch 00018: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0973 - accuracy: 0.3521 - val_loss: 1.0988 - val_accuracy: 0.3207\n",
      "Epoch 19/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0973 - accuracy: 0.3521\n",
      "Epoch 00019: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0973 - accuracy: 0.3521 - val_loss: 1.0989 - val_accuracy: 0.3207\n",
      "Epoch 20/500\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 1.0975 - accuracy: 0.3493\n",
      "Epoch 00020: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0973 - accuracy: 0.3521 - val_loss: 1.0988 - val_accuracy: 0.3207\n",
      "Epoch 21/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0973 - accuracy: 0.3521\n",
      "Epoch 00021: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0973 - accuracy: 0.3521 - val_loss: 1.0989 - val_accuracy: 0.3207\n",
      "Epoch 22/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0973 - accuracy: 0.3521\n",
      "Epoch 00022: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0973 - accuracy: 0.3521 - val_loss: 1.0989 - val_accuracy: 0.3207\n",
      "Epoch 23/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0973 - accuracy: 0.3521\n",
      "Epoch 00023: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0973 - accuracy: 0.3521 - val_loss: 1.0990 - val_accuracy: 0.3207\n",
      "Epoch 24/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0973 - accuracy: 0.3521\n",
      "Epoch 00024: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0973 - accuracy: 0.3521 - val_loss: 1.0989 - val_accuracy: 0.3207\n",
      "Epoch 25/500\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 1.0976 - accuracy: 0.3516\n",
      "Epoch 00025: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0972 - accuracy: 0.3521 - val_loss: 1.0989 - val_accuracy: 0.3207\n",
      "Epoch 26/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0973 - accuracy: 0.3521\n",
      "Epoch 00026: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0973 - accuracy: 0.3521 - val_loss: 1.0989 - val_accuracy: 0.3207\n",
      "Epoch 27/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0973 - accuracy: 0.3521\n",
      "Epoch 00027: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0973 - accuracy: 0.3521 - val_loss: 1.0989 - val_accuracy: 0.3207\n",
      "Epoch 28/500\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 1.0968 - accuracy: 0.3549\n",
      "Epoch 00028: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0973 - accuracy: 0.3521 - val_loss: 1.0989 - val_accuracy: 0.3207\n",
      "Epoch 29/500\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0973 - accuracy: 0.3521\n",
      "Epoch 00029: val_accuracy did not improve from 0.32071\n",
      "29/29 [==============================] - 2s 67ms/step - loss: 1.0973 - accuracy: 0.3521 - val_loss: 1.0989 - val_accuracy: 0.3207\n",
      "Epoch 30/500\n",
      "22/29 [=====================>........] - ETA: 0s - loss: 1.0970 - accuracy: 0.3452"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7af7dcac3212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/forex/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/forex/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/forex/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/forex/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/forex/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/forex/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/forex/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniforge3/envs/forex/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/forex/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=n_epochs,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[early_stop, model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2bc6dffc417b633bbbc31cedd954f3e10eebcdab1341647d4de83cb692948a0c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('forex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
